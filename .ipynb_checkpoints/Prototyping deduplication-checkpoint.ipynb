{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to be able to do a couple things in this repo:\n",
    "0. checking consistancy and general sanity check\n",
    "1. deduplicate (naïve if faster)\n",
    "2. tokenisation\n",
    "3. stop-words removal\n",
    "4. others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserved for libraries\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import pool as P\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 2, 2, 0, 0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "datetime.datetime.strptime('2018-2-2', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dir\n",
    "data_dir = \"../master_thesis_data/weibo_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding all files and seperating them into dates and tweets\n",
    "all_files = os.listdir(data_dir)\n",
    "\n",
    "all_texts = [file for file in all_files if file.split('.')[0][-5:] == 'texts']\n",
    "all_dates = [file for file in all_files if file.split('.')[0][-5:] == 'dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1096, 1096, 1096)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many days of data did we actually get\n",
    "len(all_texts), len(all_dates), 365 * 3 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if they match up\n",
    "actual_dates_0 = sorted([text.split('_')[1] for text in all_texts])\n",
    "actual_dates_1 = sorted([date.split('_')[1] for date in all_dates])\n",
    "\n",
    "matches = [actual_dates_0[i] == actual_dates_1[i] for i in range(len(actual_dates_0))]\n",
    "sum(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems though we managed to scrape data from nearly all dates and the 'date' labels are correctly matched, let's check using datetime which dates we're missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting limits\n",
    "starting_date = datetime.datetime.strptime('2016-4-17', '%Y-%m-%d')\n",
    "end_date = datetime.datetime.strptime('2019-4-17', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to datetime format\n",
    "actual_dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in actual_dates_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding which dates are missing\n",
    "missing_dates = []\n",
    "while starting_date != end_date + datetime.timedelta(days = 1):\n",
    "    if starting_date not in actual_dates:\n",
    "        missing_dates.append(starting_date)\n",
    "    starting_date += datetime.timedelta(days = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we got the entire period covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's see how many tweets we managed in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting total number of tweets\n",
    "tweet_count = 0\n",
    "for file in all_texts:\n",
    "    with open(os.path.join(data_dir, file), 'rb') as handle:\n",
    "        file_holder = pickle.load(handle)\n",
    "    tweet_count += len(file_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(741389, 676.0, 444833.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_count, round(tweet_count/len(matches), 0), round(tweet_count * 0.6, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed ball park 740K tweets, or on avg 676 tweets per day, we can expect around 440K tweets in total after deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototyping deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a random period first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the sample date's text and dates\n",
    "with open(os.path.join(data_dir, sorted(all_texts)[0]), 'rb') as handle:\n",
    "    texts = pickle.load(handle)\n",
    "with open(os.path.join(data_dir, sorted(all_dates)[0]), 'rb') as handle:\n",
    "    dates = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9943342776203966"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(['人工智能' in text for text in texts])/len(texts)\n",
    "# we only have 70% of the data that mention the search term, most of the non mentionning ones should be retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recording ones we want to keep\n",
    "Mention_idx = [i for i, text in enumerate(texts) if '人工智能' in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the tweets we want\n",
    "texts = [texts[i] for i in Mention_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the dates we want\n",
    "dates = [dates[i] for i in Mention_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(702, 702)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates), len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(texts))\n",
    "\n",
    "# so unique tweets wise, we only have 573"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at naive duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "702it [00:00, 14430.79it/s]\n"
     ]
    }
   ],
   "source": [
    "clusters = []\n",
    "\n",
    "for i, tweet_0 in tqdm(enumerate(texts)):\n",
    "    if sum([i in clus for clus in clusters]) == 0:\n",
    "        clusters.append([i])\n",
    "    else:\n",
    "        continue\n",
    "    for j,tweet_1 in enumerate(texts[i+1:]):\n",
    "        if tweet_0 == tweet_1:\n",
    "            clusters[-1].append(i+j+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  16,\n",
       "  42,\n",
       "  61,\n",
       "  116,\n",
       "  224,\n",
       "  276,\n",
       "  341,\n",
       "  557,\n",
       "  671,\n",
       "  675,\n",
       "  676,\n",
       "  678,\n",
       "  679,\n",
       "  689],\n",
       " [2],\n",
       " [4],\n",
       " [6],\n",
       " [7, 20, 104, 213, 342, 365, 388, 389, 401, 449, 509, 513, 521],\n",
       " [8],\n",
       " [9],\n",
       " [10],\n",
       " [11],\n",
       " [12],\n",
       " [13],\n",
       " [14],\n",
       " [15],\n",
       " [17],\n",
       " [18],\n",
       " [19],\n",
       " [21, 282],\n",
       " [22],\n",
       " [23],\n",
       " [24],\n",
       " [25],\n",
       " [26],\n",
       " [27],\n",
       " [28],\n",
       " [29],\n",
       " [30, 199],\n",
       " [31],\n",
       " [32],\n",
       " [33],\n",
       " [34],\n",
       " [35],\n",
       " [36],\n",
       " [37],\n",
       " [38],\n",
       " [39],\n",
       " [40],\n",
       " [41],\n",
       " [43],\n",
       " [44],\n",
       " [45],\n",
       " [46],\n",
       " [47],\n",
       " [48],\n",
       " [49],\n",
       " [50],\n",
       " [51],\n",
       " [52],\n",
       " [53, 284],\n",
       " [54],\n",
       " [55],\n",
       " [56],\n",
       " [57],\n",
       " [58],\n",
       " [59],\n",
       " [60],\n",
       " [62],\n",
       " [63],\n",
       " [64],\n",
       " [65],\n",
       " [66],\n",
       " [67],\n",
       " [68],\n",
       " [69],\n",
       " [70],\n",
       " [71],\n",
       " [72],\n",
       " [73],\n",
       " [74],\n",
       " [75],\n",
       " [76],\n",
       " [77],\n",
       " [78,\n",
       "  80,\n",
       "  83,\n",
       "  297,\n",
       "  311,\n",
       "  337,\n",
       "  343,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  378,\n",
       "  384,\n",
       "  392,\n",
       "  394,\n",
       "  397,\n",
       "  410,\n",
       "  422,\n",
       "  430,\n",
       "  434,\n",
       "  454,\n",
       "  462,\n",
       "  465,\n",
       "  466,\n",
       "  468,\n",
       "  480,\n",
       "  482,\n",
       "  484,\n",
       "  499,\n",
       "  502,\n",
       "  507,\n",
       "  512,\n",
       "  518,\n",
       "  523,\n",
       "  524],\n",
       " [79],\n",
       " [81, 139, 146, 168, 408, 432],\n",
       " [82],\n",
       " [84],\n",
       " [85],\n",
       " [86],\n",
       " [87],\n",
       " [88],\n",
       " [89],\n",
       " [90],\n",
       " [91],\n",
       " [92],\n",
       " [93],\n",
       " [94],\n",
       " [95],\n",
       " [96],\n",
       " [97],\n",
       " [98],\n",
       " [99],\n",
       " [100, 188, 374, 382, 407, 437, 550, 591, 594],\n",
       " [101],\n",
       " [102],\n",
       " [103],\n",
       " [105],\n",
       " [106],\n",
       " [107],\n",
       " [108],\n",
       " [109, 201, 597, 603],\n",
       " [110],\n",
       " [111],\n",
       " [112],\n",
       " [113, 319],\n",
       " [114],\n",
       " [115],\n",
       " [117],\n",
       " [118],\n",
       " [119],\n",
       " [120],\n",
       " [121],\n",
       " [122, 182, 257, 364],\n",
       " [123, 242, 546, 608],\n",
       " [124],\n",
       " [125],\n",
       " [126],\n",
       " [127],\n",
       " [128],\n",
       " [129],\n",
       " [130],\n",
       " [131, 325],\n",
       " [132],\n",
       " [133, 358],\n",
       " [134],\n",
       " [135],\n",
       " [136],\n",
       " [137],\n",
       " [138],\n",
       " [140],\n",
       " [141],\n",
       " [142],\n",
       " [143],\n",
       " [144],\n",
       " [145],\n",
       " [147],\n",
       " [148],\n",
       " [149],\n",
       " [150, 178],\n",
       " [151],\n",
       " [152],\n",
       " [153],\n",
       " [154],\n",
       " [155],\n",
       " [156, 590],\n",
       " [157],\n",
       " [158, 281, 442, 469, 501, 609, 627],\n",
       " [159],\n",
       " [160],\n",
       " [161],\n",
       " [162],\n",
       " [163],\n",
       " [164],\n",
       " [165],\n",
       " [166],\n",
       " [167],\n",
       " [169],\n",
       " [170],\n",
       " [171],\n",
       " [172],\n",
       " [173],\n",
       " [174],\n",
       " [175],\n",
       " [176],\n",
       " [177],\n",
       " [179],\n",
       " [180],\n",
       " [181],\n",
       " [183],\n",
       " [184],\n",
       " [185],\n",
       " [186],\n",
       " [187],\n",
       " [189],\n",
       " [190],\n",
       " [191],\n",
       " [192, 207, 217],\n",
       " [193],\n",
       " [194],\n",
       " [195],\n",
       " [196],\n",
       " [197],\n",
       " [198],\n",
       " [200, 222],\n",
       " [202],\n",
       " [203],\n",
       " [204],\n",
       " [205, 330],\n",
       " [206],\n",
       " [208],\n",
       " [209],\n",
       " [210, 251],\n",
       " [211],\n",
       " [212],\n",
       " [214],\n",
       " [215],\n",
       " [216],\n",
       " [218],\n",
       " [219],\n",
       " [220],\n",
       " [221],\n",
       " [223],\n",
       " [225],\n",
       " [226],\n",
       " [227],\n",
       " [228],\n",
       " [229],\n",
       " [230],\n",
       " [231],\n",
       " [232],\n",
       " [233],\n",
       " [234, 240, 252],\n",
       " [235],\n",
       " [236],\n",
       " [237],\n",
       " [238],\n",
       " [239],\n",
       " [241],\n",
       " [243],\n",
       " [244],\n",
       " [245],\n",
       " [246],\n",
       " [247],\n",
       " [248],\n",
       " [249],\n",
       " [250],\n",
       " [253],\n",
       " [254],\n",
       " [255],\n",
       " [256],\n",
       " [258],\n",
       " [259],\n",
       " [260],\n",
       " [261],\n",
       " [262, 402],\n",
       " [263],\n",
       " [264],\n",
       " [265],\n",
       " [266],\n",
       " [267, 308, 652],\n",
       " [268],\n",
       " [269],\n",
       " [270],\n",
       " [271],\n",
       " [272],\n",
       " [273],\n",
       " [274],\n",
       " [275],\n",
       " [277],\n",
       " [278],\n",
       " [279],\n",
       " [280],\n",
       " [283],\n",
       " [285],\n",
       " [286],\n",
       " [287, 503, 631],\n",
       " [288],\n",
       " [289],\n",
       " [290],\n",
       " [291],\n",
       " [292],\n",
       " [293],\n",
       " [294],\n",
       " [295],\n",
       " [296],\n",
       " [298],\n",
       " [299, 391, 429, 605],\n",
       " [300],\n",
       " [301],\n",
       " [302],\n",
       " [303],\n",
       " [304],\n",
       " [305],\n",
       " [306],\n",
       " [307],\n",
       " [309],\n",
       " [310, 625],\n",
       " [312],\n",
       " [313],\n",
       " [314],\n",
       " [315, 447],\n",
       " [316],\n",
       " [317],\n",
       " [318],\n",
       " [320],\n",
       " [321],\n",
       " [322],\n",
       " [323],\n",
       " [324],\n",
       " [326],\n",
       " [327],\n",
       " [328],\n",
       " [329],\n",
       " [331],\n",
       " [332],\n",
       " [333],\n",
       " [334],\n",
       " [335],\n",
       " [336],\n",
       " [338],\n",
       " [339],\n",
       " [340],\n",
       " [344],\n",
       " [345],\n",
       " [346],\n",
       " [350],\n",
       " [351],\n",
       " [352],\n",
       " [353],\n",
       " [354],\n",
       " [355],\n",
       " [356],\n",
       " [357, 467],\n",
       " [359],\n",
       " [360],\n",
       " [361],\n",
       " [362],\n",
       " [363],\n",
       " [366, 636],\n",
       " [367],\n",
       " [368],\n",
       " [369],\n",
       " [370],\n",
       " [371],\n",
       " [372],\n",
       " [373],\n",
       " [375],\n",
       " [376],\n",
       " [377],\n",
       " [379],\n",
       " [380, 441],\n",
       " [381],\n",
       " [383],\n",
       " [385],\n",
       " [386],\n",
       " [387],\n",
       " [390],\n",
       " [393],\n",
       " [395],\n",
       " [396],\n",
       " [398],\n",
       " [399],\n",
       " [400],\n",
       " [403],\n",
       " [404],\n",
       " [405],\n",
       " [406, 487],\n",
       " [409],\n",
       " [411],\n",
       " [412],\n",
       " [413],\n",
       " [414],\n",
       " [415],\n",
       " [416],\n",
       " [417],\n",
       " [418],\n",
       " [419],\n",
       " [420],\n",
       " [421],\n",
       " [423],\n",
       " [424],\n",
       " [425],\n",
       " [426],\n",
       " [427],\n",
       " [428],\n",
       " [431],\n",
       " [433],\n",
       " [435],\n",
       " [436],\n",
       " [438],\n",
       " [439],\n",
       " [440],\n",
       " [443],\n",
       " [444],\n",
       " [445],\n",
       " [446],\n",
       " [448],\n",
       " [450],\n",
       " [451],\n",
       " [452],\n",
       " [453],\n",
       " [455],\n",
       " [456],\n",
       " [457, 535, 548, 564, 568, 575],\n",
       " [458, 569, 576],\n",
       " [459],\n",
       " [460],\n",
       " [461],\n",
       " [463],\n",
       " [464],\n",
       " [470],\n",
       " [471],\n",
       " [472],\n",
       " [473],\n",
       " [474],\n",
       " [475],\n",
       " [476],\n",
       " [477],\n",
       " [478],\n",
       " [479],\n",
       " [481],\n",
       " [483],\n",
       " [485],\n",
       " [486],\n",
       " [488],\n",
       " [489],\n",
       " [490],\n",
       " [491],\n",
       " [492],\n",
       " [493],\n",
       " [494],\n",
       " [495],\n",
       " [496],\n",
       " [497],\n",
       " [498],\n",
       " [500],\n",
       " [504],\n",
       " [505],\n",
       " [506],\n",
       " [508],\n",
       " [510],\n",
       " [511],\n",
       " [514],\n",
       " [515],\n",
       " [516],\n",
       " [517],\n",
       " [519],\n",
       " [520],\n",
       " [522],\n",
       " [525],\n",
       " [526],\n",
       " [527],\n",
       " [528],\n",
       " [529],\n",
       " [530],\n",
       " [531],\n",
       " [532, 607],\n",
       " [533],\n",
       " [534],\n",
       " [536, 549],\n",
       " [537],\n",
       " [538],\n",
       " [539, 562],\n",
       " [540],\n",
       " [541],\n",
       " [542],\n",
       " [543],\n",
       " [544],\n",
       " [545],\n",
       " [547],\n",
       " [551],\n",
       " [552],\n",
       " [553],\n",
       " [554],\n",
       " [555],\n",
       " [556],\n",
       " [558],\n",
       " [559],\n",
       " [560],\n",
       " [561],\n",
       " [563],\n",
       " [565],\n",
       " [566],\n",
       " [567],\n",
       " [570],\n",
       " [571],\n",
       " [572],\n",
       " [573],\n",
       " [574],\n",
       " [577],\n",
       " [578],\n",
       " [579],\n",
       " [580],\n",
       " [581],\n",
       " [582],\n",
       " [583],\n",
       " [584],\n",
       " [585],\n",
       " [586],\n",
       " [587],\n",
       " [588],\n",
       " [589],\n",
       " [592],\n",
       " [593],\n",
       " [595],\n",
       " [596],\n",
       " [598],\n",
       " [599],\n",
       " [600],\n",
       " [601],\n",
       " [602],\n",
       " [604],\n",
       " [606],\n",
       " [610],\n",
       " [611],\n",
       " [612],\n",
       " [613],\n",
       " [614],\n",
       " [615],\n",
       " [616],\n",
       " [617],\n",
       " [618],\n",
       " [619],\n",
       " [620],\n",
       " [621],\n",
       " [622],\n",
       " [623],\n",
       " [624],\n",
       " [626],\n",
       " [628],\n",
       " [629],\n",
       " [630],\n",
       " [632],\n",
       " [633],\n",
       " [634],\n",
       " [635],\n",
       " [637],\n",
       " [638],\n",
       " [639],\n",
       " [640],\n",
       " [641],\n",
       " [642],\n",
       " [643],\n",
       " [644],\n",
       " [645],\n",
       " [646],\n",
       " [647],\n",
       " [648],\n",
       " [649],\n",
       " [650],\n",
       " [651],\n",
       " [653],\n",
       " [654],\n",
       " [655],\n",
       " [656],\n",
       " [657],\n",
       " [658],\n",
       " [659],\n",
       " [660],\n",
       " [661],\n",
       " [662],\n",
       " [663],\n",
       " [664],\n",
       " [665],\n",
       " [666],\n",
       " [667],\n",
       " [668],\n",
       " [669],\n",
       " [670],\n",
       " [672],\n",
       " [673],\n",
       " [674],\n",
       " [677],\n",
       " [680],\n",
       " [681],\n",
       " [682],\n",
       " [683],\n",
       " [684],\n",
       " [685],\n",
       " [686],\n",
       " [687],\n",
       " [688],\n",
       " [690],\n",
       " [691],\n",
       " [692],\n",
       " [693],\n",
       " [694],\n",
       " [695],\n",
       " [696],\n",
       " [697],\n",
       " [698],\n",
       " [699],\n",
       " [700],\n",
       " [701]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we manually check of what nature they are, we'll sample a few\n",
    "clusters = [clus for clus in clusters if len(clus) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#小米电视3s#人工智能电视，蜕变真实的你！\n",
      "----------------------\n",
      " \n",
      "#小米电视3s#人工智能电视，蜕变真实的你！\n",
      "----------------------\n",
      " \n",
      "#小米电视3s#人工智能电视，蜕变真实的你！\n",
      "----------------------\n",
      " \n",
      "#小米电视3s#人工智能电视，蜕变真实的你！\n",
      "----------------------\n",
      " \n",
      "#小米电视3s#人工智能电视，蜕变真实的你！\n",
      "----------------------\n",
      " \n",
      "#小米电视3s#人工智能电视，蜕变真实的你！\n",
      "----------------------\n",
      " \n",
      "#小米电视3s#人工智能电视，蜕变真实的你！\n",
      "----------------------\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for idx in random.choice(clusters):\n",
    "    print(texts[idx])\n",
    "    print('----------------------')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK so let's now go on away and execute a naïve deduplication now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
